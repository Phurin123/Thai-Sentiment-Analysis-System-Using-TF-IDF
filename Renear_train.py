# train.py
import re
import pandas as pd
import uuid
from datetime import datetime

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
    f1_score,
)
import joblib
from sklearn.svm import LinearSVC
import matplotlib.pyplot as plt
import seaborn as sns

import os
os.makedirs("models_linear", exist_ok=True)

# === 1. Preprocessing function ===
def preprocess(text):
    """
    ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ preprocessing:
    - ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤-‡∏´‡∏•‡∏±‡∏á: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡πà‡∏≤ null ‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏ö‡∏Å‡∏ß‡∏ô
    - normalize whitespace: ‡∏£‡∏ß‡∏°‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≠‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‚Üí ‡∏•‡∏î noise ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏°‡∏û‡πå
    - ‡πÑ‡∏°‡πà lowercase ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÉ‡∏´‡∏ç‡πà/‡πÄ‡∏•‡πá‡∏Å
    - ‡πÑ‡∏°‡πà‡∏•‡∏ö emoji/slang ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏´‡πâ‡∏≤‡∏° over-cleaning
    """
    text = str(text).strip()
    text = re.sub(r"\s+", " ", text)  # normalize whitespace
    return text


# === 2. Load and prepare data ===
df = pd.read_csv("data/5.ultimate_sentiment_100k.csv")
df = df.rename(columns={"sentiment": "label"})
df["text"] = df["text"].apply(preprocess)

# === 3. Train-test split ===
X = df["text"]
y = df["label"]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# === 4. Train model (WORD-LEVEL TF-IDF ‡∏ï‡∏≤‡∏° requirement) ===
vectorizer = TfidfVectorizer(
    analyzer="word",        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô word-level ‡∏ï‡∏≤‡∏°‡πÇ‡∏à‡∏ó‡∏¢‡πå
    ngram_range=(1, 2),     # unigram + bigram ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö context ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
    max_features=10000      # ‡∏à‡∏≥‡∏Å‡∏±‡∏î feature ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏•‡∏î overfit
)
X_train_vec = vectorizer.fit_transform(X_train)
model = LinearSVC(
    class_weight="balanced",
    random_state=42
)
model.fit(X_train_vec, y_train)
model.fit(X_train_vec, y_train)

# === 5. Generate UID for model version ===
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
uid = uuid.uuid4().hex[:8]
model_uid = f"{timestamp}_{uid}"

# === 6. Save model and vectorizer ===
model_path = f"models_linear/sentiment_model_{model_uid}.joblib"
vectorizer_path = f"models_linear/vectorizer_{model_uid}.joblib"

joblib.dump(model, model_path)
joblib.dump(vectorizer, vectorizer_path)

print(f"Model saved as: {model_path}")
print(f"Vectorizer saved as: {vectorizer_path}")

# === 7. Evaluation ===
X_test_vec = vectorizer.transform(X_test)
y_pred = model.predict(X_test_vec)

acc = accuracy_score(y_test, y_pred)
f1_macro = f1_score(y_test, y_pred, average="macro")
cm = confusion_matrix(y_test, y_pred)

print("\n=== EVALUATION RESULTS ===")
print("Accuracy:", round(acc, 4))
print("Macro-F1:", round(f1_macro, 4))
print("Confusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# === 7.1 Save evaluation results as image ===
os.makedirs("results_linear", exist_ok=True)
results_path = f"results_linear/evaluation_{model_uid}.png"

# ‡∏™‡∏£‡πâ‡∏≤‡∏á figure
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# --- ‡∏ã‡πâ‡∏≤‡∏¢: Metrics as text ---
axes[0].axis('off')
metrics_text = (
    f"Model UID: {model_uid}\n\n"
    f"Classifier: LinearSVC\n"
    f"Accuracy: {acc:.4f}\n"
    f"Macro-F1:  {f1_macro:.4f}"
)
axes[0].text(0.1, 0.5, metrics_text, fontsize=14, verticalalignment='center', fontfamily='monospace')

# --- ‡∏Ç‡∏ß‡∏≤: Confusion Matrix ---
sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    ax=axes[1],
    xticklabels=model.classes_,
    yticklabels=model.classes_
)
axes[1].set_title('Confusion Matrix')
axes[1].set_xlabel('Predicted Label')
axes[1].set_ylabel('True Label')

plt.tight_layout()
plt.savefig(results_path, dpi=150, bbox_inches='tight')
plt.close()

print(f"‚úÖ Saved evaluation results as image: {results_path}")

# === 8. Show ‚â•10 misclassified examples ===
test_indices = X_test.index
errors_df = df.loc[test_indices].copy()
errors_df["true_label"] = y_test.values
errors_df["pred_label"] = y_pred

print("\n=== 10 MISCLASSIFIED EXAMPLES ===")
for idx, (_, row) in enumerate(errors_df.head(10).iterrows()):
    print(f"{idx+1}. Text: {row['text']}")
    print(f"    True: {row['true_label']} | Pred: {row['pred_label']}\n")

# === 9. Error Analysis (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ ‚â•3 ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó) ===
def categorize_error(text, true_label, pred_label):
    text_lower = text.lower()
    # 1. Negation / Mixed signal
    neg_words = ["‡πÑ‡∏°‡πà", "‡πÅ‡∏°‡πà‡∏á", "‡πÅ‡∏¢‡πà‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏¥‡∏î", "‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á", "‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á"]
    pos_words = ["‡∏î‡∏µ‡∏°‡∏≤‡∏Å", "‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à", "‡πÇ‡∏≠‡πÄ‡∏Ñ‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ñ‡∏≤‡∏î", "‡∏ä‡∏≠‡∏ö"]
    
    has_neg = any(w in text for w in neg_words)
    has_pos = any(w in text for w in pos_words)
    
    if has_neg and has_pos:
        return "Mixed Signal / Ambiguity"
    
    # 2. Sarcasm / Informal tone (‡πÄ‡∏ä‡πà‡∏ô ‡πÉ‡∏ä‡πâ emoji ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏´‡∏¢‡∏≤‡∏ö‡πÅ‡∏ï‡πà sentiment ‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î)
    if any(emoji in text for emoji in ["üò§", "üôÑ", "üòí", "üôÇ", "üòä"]) or "‡πÅ‡∏°‡πà‡∏á" in text:
        return "Sarcasm / Informal Expression"
    
    # 3. Domain-specific ambiguity (‡πÄ‡∏ä‡πà‡∏ô ‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á "‡∏£‡∏∞‡∏ö‡∏ö" ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ß‡πà‡∏≤‡∏î‡∏µ/‡πÅ‡∏¢‡πà)
    neutral_phrases = ["‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÇ‡∏î‡∏î‡πÄ‡∏î‡πà‡∏ô", "‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥", "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏¢‡πà ‡πÅ‡∏ï‡πà‡∏Å‡πá‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏î‡∏µ"]
    if any(phrase in text for phrase in neutral_phrases):
        return "Ambiguous Neutral Expression"
    
    # Default
    return "Other"

# ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏° error
errors_df["error_type"] = errors_df.apply(
    lambda row: categorize_error(row["text"], row["true_label"], row["pred_label"]), axis=1
)

print("\n=== ERROR ANALYSIS (Grouped by Type) ===")
error_counts = errors_df["error_type"].value_counts()
print(error_counts)

most_common_error = error_counts.idxmax()
print(f"\nMost common error type: '{most_common_error}' ({error_counts[most_common_error]} cases)")

# ‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç
if most_common_error == "Mixed Signal / Ambiguity":
    suggestion = "‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£ detect negation ‡πÅ‡∏•‡∏∞ contextual cues ‡∏î‡πâ‡∏ß‡∏¢ rule-based ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• sequence ‡πÄ‡∏ä‡πà‡∏ô BERT"
elif most_common_error == "Sarcasm / Informal Expression":
    suggestion = "‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£ normalize emoji ‡πÅ‡∏•‡∏∞ slang ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏°‡∏±‡∏î‡∏£‡∏∞‡∏ß‡∏±‡∏á ‡∏´‡∏£‡∏∑‡∏≠ fine-tune ‡∏ö‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• informal Thai"
elif most_common_error == "Ambiguous Neutral Expression":
    suggestion = "‡∏™‡∏£‡πâ‡∏≤‡∏á class 'Neutral' ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ threshold-based confidence ‡πÅ‡∏ó‡∏ô hard label"
else:
    suggestion = "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏•‡∏∂‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô transformer-based model"

print(f"\nSuggested improvement: {suggestion}")